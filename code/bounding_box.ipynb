{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data + YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/npenoyer34/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-3-19 Python-3.10.13 torch-2.1.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "raw_imgs_path = '../data/raw'\n",
    "all_imgs = glob.glob(os.path.join(raw_imgs_path, '*.jpg'))\n",
    "\n",
    "### DOWNSAMPLED FOR TESTING ###\n",
    "# sample 10\n",
    "# sampled_imgs = all_imgs[:10]\n",
    "\n",
    "# # load imgs\n",
    "#imgs = [Image.open(img_path) for img_path in sampled_imgs]\n",
    "\n",
    "### THE REAL DEAL ###\n",
    "# load imgs\n",
    "imgs = [Image.open(img_path) for img_path in all_imgs]\n",
    "\n",
    "# load yolo\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(imgs)\n",
    "\n",
    "# sanity check\n",
    "# results.print()\n",
    "\n",
    "dets = results.pandas().xyxy\n",
    "\n",
    "# only keep dataframes and images with at least 2 subjects\n",
    "filtered_dets = [det for det in dets if len(det) >= 2]\n",
    "filtered_all_imgs = [img_path for img_path, det in zip(all_imgs, dets) if len(det) >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img_paths, detections_list, verbose=False):\n",
    "  out_path = '../data/tightbox'\n",
    "  os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "  for img_path, detections in zip(img_paths, detections_list):\n",
    "    # load img\n",
    "    img = Image.open(img_path)\n",
    "    # find the bounding box that contains all objects\n",
    "    xmin = int(detections['xmin'].min())\n",
    "    ymin = int(detections['ymin'].min())\n",
    "    xmax = int(detections['xmax'].max())\n",
    "    ymax = int(detections['ymax'].max())\n",
    "\n",
    "    #find new area and see if it is sufficiently smaller than original area\n",
    "    size=img.size[0]*img.size[1]\n",
    "    cropped_size=(xmax-xmin)*(ymax-ymin)\n",
    "    \n",
    "    if  cropped_size < .75*size:\n",
    "\n",
    "\n",
    "      #crop image around bounding box\n",
    "      cropped_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "\n",
    "      # get image name; remove extension\n",
    "      img_name = os.path.basename(img_path)\n",
    "      img_name_no_ext = os.path.splitext(img_name)[0]\n",
    "\n",
    "      # create new image name w/ bounding box coords\n",
    "      new_img_name = f\"{img_name_no_ext}_subject_box_{xmin}_{ymin}_{xmax}_{ymax}.jpg\"\n",
    "\n",
    "      # save cropped img\n",
    "      save_path = os.path.join(out_path, new_img_name)\n",
    "      cropped_img.save(save_path)\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_and_save(filtered_all_imgs, filtered_dets, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
